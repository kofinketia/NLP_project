{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: backoff in c:\\users\\danie\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (2.2.1)\n"
     ]
    }
   ],
   "source": [
    "! pip install backoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\danie\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "from gpt_index import SimpleDirectoryReader, GPTListIndex, GPTSimpleVectorIndex, LLMPredictor, PromptHelper, ServiceContext\n",
    "from langchain import OpenAI\n",
    "import sys\n",
    "import os\n",
    "from gtts import gTTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and concatenate datasets\n",
    "dataset_paths = ['dataset.csv', 'Language Detection.csv']\n",
    "dataframes = [pd.read_csv(path) for path in dataset_paths]\n",
    "df = pd.concat(dataframes)\n",
    "\n",
    "# Convert language labels to integers\n",
    "label_encoder = LabelEncoder()\n",
    "df['label'] = label_encoder.fit_transform(df['Language'])\n",
    "\n",
    "# Load the DistilBert tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-multilingual-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-multilingual-cased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the DistilBertForSequenceClassification model\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-multilingual-cased', num_labels=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=30, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the saved model\n",
    "path = \"model_best50.pt\"\n",
    "model.load_state_dict(torch.load(path, map_location=torch.device('cpu')))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai.error import RateLimitError\n",
    "import backoff\n",
    "\n",
    "@backoff.on_exception(backoff.expo, RateLimitError)\n",
    "def completions_with_backoff(**kwargs):\n",
    "    response = openai.Completion.create(**kwargs)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_index(directory_path):\n",
    "    max_input_size = 4096\n",
    "    num_outputs = 512\n",
    "    max_chunk_overlap = 20\n",
    "    chunk_size_limit = 600\n",
    "\n",
    "    prompt_helper = PromptHelper(max_input_size, num_outputs, max_chunk_overlap, chunk_size_limit=chunk_size_limit)\n",
    "\n",
    "    llm_predictor = LLMPredictor(llm=OpenAI(temperature=0.7, model_name=\"text-davinci-003\", max_tokens=num_outputs))\n",
    "\n",
    "    documents = SimpleDirectoryReader(directory_path).load_data()\n",
    "    service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, prompt_helper=prompt_helper)\n",
    "\n",
    "    index = GPTSimpleVectorIndex.from_documents(documents=documents, service_context=service_context)\n",
    "    index.save_to_disk('chat.json')\n",
    "\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gpt_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "INFO:gpt_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 8973260 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<gpt_index.indices.vector_store.vector_indices.GPTSimpleVectorIndex at 0x2565b2f75b0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = construct_index(\"Docs\")\n",
    "index.load_from_disk('chat.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_dict = {\"English\" : \"en\",\n",
    "             \"Spanish\" : \"es\",\n",
    "             \"German\" : \"de\",\n",
    "             \"French\" : \"fr\",\n",
    "             \"Italian\" : \"it\",\n",
    "             \"Portuguese\" : \"pt\",\n",
    "             \"Dutch\" : \"nl\",\n",
    "             \"Russian\" : \"ru\",\n",
    "             \"Arabic\" : \"ar\",\n",
    "             \"Japanese\" : \"ja\",\n",
    "             \"Korean\" : \"ko\",\n",
    "             \"Chinese\" : \"zh-CN\"\n",
    "            }  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatbot(text):\n",
    "# Check if input text contains the word \"language\"\n",
    "  if 'what language is this:' in text.lower():\n",
    "      # Use the language detection model to detect the language\n",
    "      text = text.split(\"what language is this:\")[1].strip()\n",
    "      encoding = tokenizer.encode_plus(text, padding=True, truncation=True, return_tensors='pt')\n",
    "      input_ids = encoding['input_ids']\n",
    "      attention_mask = encoding['attention_mask']\n",
    "\n",
    "      with torch.no_grad():\n",
    "          output = model(input_ids, attention_mask=attention_mask)\n",
    "          probabilities = torch.nn.functional.softmax(output[0], dim=1)\n",
    "          _, predicted = torch.max(output[0], dim=1)\n",
    "          predicted_language = label_encoder.inverse_transform(predicted.cpu().numpy())\n",
    "\n",
    "      # Return the detected language\n",
    "      response = f\"The detected language is {predicted_language[0]}\"\n",
    "      language = lang_dict[predicted_language[0]]\n",
    "      \n",
    "      #Getting to speech\n",
    "      myobj = gTTS(text=response, lang='en', slow=False) \n",
    "      myobj.save(\"test.wav\")\n",
    "      myobj2 = gTTS(text=text, lang=language, slow=False) \n",
    "      myobj2.save(\"test2.wav\") \n",
    "      return 'test.wav', 'test2.wav'\n",
    "  else:\n",
    "      # Use the GPT-based chatbot to generate a response\n",
    "      response = index.query(text, response_mode=\"compact\").response\n",
    "      \n",
    "      # Predict language of the output\n",
    "      encoding = tokenizer.encode_plus(response, padding=True, truncation=True, return_tensors='pt')\n",
    "      input_ids = encoding['input_ids']\n",
    "      attention_mask = encoding['attention_mask']\n",
    "\n",
    "      with torch.no_grad():\n",
    "          output = model(input_ids, attention_mask=attention_mask)\n",
    "          probabilities = torch.nn.functional.softmax(output[0], dim=1)\n",
    "          _, predicted = torch.max(output[0], dim=1)\n",
    "          predicted_language = label_encoder.inverse_transform(predicted.cpu().numpy())\n",
    "          \n",
    "      language = lang_dict[predicted_language[0]]\n",
    "      \n",
    "      #Getting to speech\n",
    "      myobj = gTTS(text=response, lang=language, slow=False) \n",
    "      myobj.save(\"test.wav\")\n",
    "      myobj2 = gTTS(text=text, lang=language, slow=False) \n",
    "      myobj2.save(\"test2.wav\") \n",
    "      return 'test.wav', 'test2.wav'\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7864\n",
      "Running on public URL: https://9337b92f13d65a2029.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades (NEW!), check out Spaces: https://huggingface.co/spaces\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://9337b92f13d65a2029.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gpt_index.token_counter.token_counter:> [query] Total LLM token usage: 3798 tokens\n",
      "INFO:gpt_index.token_counter.token_counter:> [query] Total embedding token usage: 5 tokens\n",
      "INFO:gpt_index.token_counter.token_counter:> [query] Total LLM token usage: 3746 tokens\n",
      "INFO:gpt_index.token_counter.token_counter:> [query] Total embedding token usage: 6 tokens\n",
      "INFO:gpt_index.token_counter.token_counter:> [query] Total LLM token usage: 3919 tokens\n",
      "INFO:gpt_index.token_counter.token_counter:> [query] Total embedding token usage: 5 tokens\n",
      "INFO:gpt_index.token_counter.token_counter:> [query] Total LLM token usage: 3931 tokens\n",
      "INFO:gpt_index.token_counter.token_counter:> [query] Total embedding token usage: 5 tokens\n",
      "INFO:gpt_index.token_counter.token_counter:> [query] Total LLM token usage: 3883 tokens\n",
      "INFO:gpt_index.token_counter.token_counter:> [query] Total embedding token usage: 4 tokens\n",
      "INFO:gpt_index.token_counter.token_counter:> [query] Total LLM token usage: 3781 tokens\n",
      "INFO:gpt_index.token_counter.token_counter:> [query] Total embedding token usage: 4 tokens\n",
      "INFO:gpt_index.token_counter.token_counter:> [query] Total LLM token usage: 3929 tokens\n",
      "INFO:gpt_index.token_counter.token_counter:> [query] Total embedding token usage: 5 tokens\n",
      "INFO:gpt_index.token_counter.token_counter:> [query] Total LLM token usage: 3801 tokens\n",
      "INFO:gpt_index.token_counter.token_counter:> [query] Total embedding token usage: 5 tokens\n",
      "INFO:gpt_index.token_counter.token_counter:> [query] Total LLM token usage: 3792 tokens\n",
      "INFO:gpt_index.token_counter.token_counter:> [query] Total embedding token usage: 4 tokens\n"
     ]
    }
   ],
   "source": [
    "title = \"Chatbot for Languages\"\n",
    "description = \"This app can detect the language of your input text and generate a response. It includes up to 30 languages such as English, Spanish, French, German, Italian, Portuguese, Dutch, Russian, Arabic, Chinese, Japanese,Korean, and many more.\"\n",
    "examples = [\"what language is this: How are you\",\n",
    "            \"what language is this: Comment allez-vous\",\n",
    "            \"what language is this: Cómo estás\",\n",
    "            \"what language is this: Wie geht es dir\",\n",
    "            \"what language is this: 어떻게 지내세요\",\n",
    "            \"what language is this: どうもありがとうございます\",\n",
    "            \"what language is this: Hoe is het\",\n",
    "            \"what language is this: Come stai\",\n",
    "            \"what language is this: 你好吗\",\n",
    "            \"what language is this: 元気ですか\",\n",
    "            \"what language is this: كيف حالك\" \n",
    "            ]\n",
    "\n",
    "\n",
    "iface = gr.Interface(fn = chatbot,\n",
    "                     inputs = gr.Textbox(label=\"Please type text here\"),\n",
    "                     outputs = [gr.Audio(label=\"Output speech\"), gr.Audio(label=\"Reading input text\")], \n",
    "                     #verbose = True,\n",
    "                     title = title,\n",
    "                     description = description,\n",
    "                     article = \n",
    "                        '''<div>\n",
    "                            <p style=\"text-align: center\"> All you need to do is to type a text in any of the listed language. Then click on Play/Pause to hear the name of the language. The audio is saved in a wav format.</p>\n",
    "                        </div>''',\n",
    "                     examples=examples\n",
    "                    )\n",
    "iface.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
